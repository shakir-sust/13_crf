---
title: "Random forest"
format: html
---

# Learning objectives  
Our learning objectives are to:  
  - Understand random forest algorithm 
  - Use the ML framework to:  
    - pre-process data
    - train a random forest model 
    - evaluate model predictability 
  - Explore a few new concepts:  
    - Selecting **best model within one pct loss**  
    

## Will come in exam: 
- What is multicollinearity? 
- How to overcome multicollinearity? 
- What is a supervised and unsupervised model? 
- Name some supervised and unsupervised models?
- Bias-variance trade-off plot
- What does last fit mean? 

# Introduction  
As we previously learned, linear regression models can suffer from **multicollinearity** when two or more predictor variables are highly correlated.  

The methods we mentioned to overcome multicollinearity include:  
  - Dimensionality reduction (e.g., PCA)  
  - Variable selection:
    - by hand (using expert knowledge to determine what predictors should be included in the model)
    - by models (e.g., random forest, CIT, elastic net) 
    
[Note: Some Machine Learning models (not all of them) handle multicollinearity well. 

- Elastic net handles multicollinearity by finding variables that are correlated and basically forces one of these variables to have 0 slopeon the model by managing both L1 and L2 penalties.  

- Conditional inference tree handles multicollinearity -- It never truly evaluates all variables together -- it evaluates predictor variables one-by-one at each one of the splits. So, it's always doing like a strength by one weather variable in our case and then strength by another one, and strength by another one, and then getting the p value of each one of them choosing the lowest p value to keep moving forward. So, although it is a multivariate analysis, but it never really sees all variables together when it's doing the linear regression part of the model. That's how it handles multicollinearity -- by avoiding having multicollinear variables in a given modeling step. 

- Random forest also handles multicollinearity, but in a different way. ]

Today, we'll explore another model that addresses multicolinearity: **random forest**.  

## Random forest 
Random forest is a very [robust] popular  **supervised** “out-of-the-box” or “off-the-shelf” learning algorithm that has good predictive performance with relatively little hyperparameter tuning.  [meaning even if we do not conduct hyperparameter fine tuning, random forest is still going to be a very reliable option. However, we should always do hyperparameter fine tuning.]

The power of random forests come from **two random processes**:  
  - Bootstrap aggregating (bagging) on rows  
  - Random selection of a subset of predictor variables (i.e., columns) at each split. [Random selection of a subset of features/predictor variables/columns at each split]  
  
[ Very important note: 

*Why is random forest is such a powerful/robust machine learning algorithm?*

The power of random forests come from these two random processes working together:

- Random selection of rows with bootstrap aggregating (bagging) [random bootstrap sample of the rows for each tree]
- Random selection of a subset of predictor variables/ features/ columns at each split in each tree [random subset of columns for each split in each tree]

Each tree is seeing a different version of our data, and each split in the tree is seeing a different set of predictor variables/features/columns.

So, in the end when you ask for a prediction from this forest, each tree is giving a prediction. And maybe if you were to look at just one of those trees, it would be a horrible prediction. But because you have so many trees, and they were all trained on a different version of our data, they make a really good prediction on average. That's the power of random forest and that's how it works so well. 
]
  
## Bootstrap aggregating (bagging)  
Bagging trees introduces a random component into the tree building process by building **many trees on bootstrapped copies of the training data**.   

Bagging then aggregates the predictions across all the trees; this aggregation reduces the variance of the overall procedure and results in improved predictive performance.

## Random subset of features [at each split on each tree]  
While growing a decision tree during the bagging process, random forests perform split-variable randomization where each time a split is to be performed, the search for the split variable is limited to a random subset of **mtry** of the original **p** features. 

[Note:
Difference between CIT and random forest:
With CIT we have only 1 tree, whereas with random forest we can have as many trees as we ask e.g., 1000 tress (that's why it's call "forest")
]

Typical default values for mtry are **p/3** for regression (our case; when y/predicted variable is numerical) and $\sqrt p$ for classification (when y/predicted variable is categorical).  

[*mtry  = number of features that we allow for each split and each tree to see.* 
We have 73 number of features. So, p = 73. 
So, the default mtry value will be = 73/3 = 24.3333. So, 24.3333 would be the number of columns that we randomly selected at each split of each tree.
So, each split would see a random subset of 24 columns only (by default). If we just run the random forest and not train it, this would be the default random subset (of 24 columns) that it would use in our case. But this value is trainable, we can further improve our model by finding the best value of mtry. 
]

Since the algorithm **randomly selects** a **bootstrap sample** [of rows] to train on and a **random sample of features to use at each split** [of each tree], a more diverse set of trees is produced which tends to lessen tree correlation beyond bagged trees and often dramatically increase predictive power.  

So, how can we control the simplicity/complexity of the tree?  

**Training a model by fine-tuning its hyper-parameters**.

There will be 3 main hyperparameters that we can fine-tune:  
  - **number of trees in the forest**: as the name says, how many individual trees will be trained  [how many individual trees that we would allow that forest to grow]
  - **number of features randomly selected at each split [of each tree] (mtry)**: as the name says  
  - **minimum node size**:  minimum number of observations in a node [of a given tree] for it to be split.  
  
## Number of trees  
The number of trees needs to be sufficiently large to stabilize the error rate. 
A good rule of thumb is to start with 10 times the number of features [features = variables/ columns].   

[Note:
Normally what we want is a forest that has enough trees that stabilize the Out-of-bag (OOB) error.

OOB Error = Out-of-bag error

Each tree is using a bootstrap sample of the training set. So, if your training set has 10 rows, the bootstrap sample has 10 rows. But the bootstrap sample may have some samples that are duplicated, and some that are completely missing. The ones that are missing are the Out-of-bag samples. So, it trains on your bootstrap sample, but it knows which ones are not present on the bootstrap set. Then it makes a prediction on those. That's what the Out-of-bag (OOB) sample or Out-of-bag (OOB) error is.

In the plot, We want a forest that has enough trees to stabilize the error. A good rule of thumb is to start with 10 times the number of features [features = variables/ columns]. In our case, we have 72 features, so we will start with 72 x 10 = 720 trees in our forest.

In this example here (the plot shown below), which does not come from our data, you see that your error on the y-axis decreases as you increase number of trees on the x-axis. But if it reaches a plateau after 500 trees. This model here evaluated up to about 1000 trees, but maybe it was already good enough at 500 trees. But how do you know that? how do you know what's the best level? by fine-tuning. The number of trees is one of the hyperparameters that we're going to fine-tune. So, in general more trees are not going to hurt your model performance. But if you have an excessive number of trees in a very complex model, why would you use 1000 tree if you only need 500 trees? Having an optimized/fine tuned number of trees is especially important because having more-than-necessary number of trees will hurt your computational time although the model performance will not be hurt. Again, although number of trees does not does not hurt your model predictive ability, but it does hurt your computational demand to train the model. That's why we need to fine-tune/optimize the number of trees.
]



![](https://bradleyboehmke.github.io/HOML/09-random-forest_files/figure-html/tuning-trees-1.png)
     
More trees provide more robust and stable error estimates and variable importance measures; however, the **impact on computation time increases linearly with the number of trees**.  
     
     
## mtry  

[mtry = the number of features each split in each tree is allowed to see ]

**mtry helps to balance low tree correlation with reasonable predictive strength.** 

When there are **fewer relevant predictors** (e.g., noisy data) a **higher value of mtry** tends to perform better because it makes it more likely to select those features with the strongest signal.   

When there are many relevant predictors, a lower mtry might perform better.

[Note:
 mtry makes sure that each split of each tree is seeing a different subset of variables, and therefore creating variability in the trees you're creating. if you were to give always the same exact set of rows and columns to all 1000 trees, you would probably have a thousand trees that are very similar because they're all being trained on the same data, they all are seeing the same data. 

Random forest is giving a a different subset of rows to the tree, and within that tree a different subset of columns that it splits, making that tree be very different from its neighboring tree or any other tree in that forest. 

So, why do we have to fine-tune mtry? When we have fewer relevant predictors, then a higher value of mry works better. Why is that? imagine that in our 72 predictors, if we only had three that were really strongly correlated with strength and everything else is not; if you have a high value of mtry of let's say 50, it means that each split is going to see a random selection of 50 columns. the chance of you having one of your three highly important variables in a subset of 50 is high. so at least you're having that very important variable appearing most of the times in most splits of most trees.

So, if you have a large number of of variables, but only a very small subset of them are really important for you, but you don't know yet, then  mtry should be big, because the chance of having those fewer set of highly important variables appearing for each split is going to be higher, making sure your trees are trained well.

Now, on the other hand, if you have many variables that are highly important in explaining your predicted variable, then you want a lower mtry value, because you don't want the the trees to see all of those very important variables all the time. You want you want to give different sets of important variables to different splits, so that they can create variability in the training process. 

Let's visualize that effect in the plot shown below.

Here, we have number of trees on the x- axis and OOB (out of bag) error as RMSE on the y- axis, and each line here is a value of mtry. As you see here, if we give an mtry of 2 (meaning that each split of each tree is only seeing 2 predictor variables. We do see that overall pattern of increasing trees are decreasing error. But if you have very few number of variables being evaluated at each splits (in this case 2), even though the OOB error stabilizes, but it remains pretty high.

Now, if you see other other lines here (that kind of all almost converge on the bottom) -- THE line that has the lowest error is where mry was 21. And then there is one line for which mtry is 80, which is not the best.

As you can see, there is a sweet spots of mtry. It is not that: "always high is going to be better", or "always low is going to be better". It's going to depend on the data, and it's probably going to fluctuate as well. 

In this case, mry of 21 (meaning that for each tree on each split it was evaluating 21 random features of your data) was the best for this data set.
]

[*Very specific example of how random forest works:*

The first random process (which is a bootstrap of the rows) is applied once for that tree. It's going to be different for the next tree, and then the next tree, and so on. But within a tree, the the subset of random rows is the same. Let's say your training set has 10 rows, you do a bootstrap sample of that. So, the bootstrap sample is also going to have 10 rows. But it could be that some are replicated and some are missing from the bootstrap sample -- that's what goes into that first tree. Now this first tree still has 72 columns. mtry is the value that's going to determine on each split how many and which one of those 72 columns are evaluated.

So, let's say that at the first split, for a given model mtry is fixed (mtry is going to be let's say 3) meaning that if mtry is 3 for that first tree of the first split, it's going to see 10 rows but only 3 columns of the 73 total and that is a random selection of which 3 columns it sees. So, it only evaluates those 3 columns to decide which one of the 3 are the best one, and then use that to make a split. And that process of selecting the best one and making a split is very similar to the conditional inference tree.

But now once it makes a break, it splits again -- now those two splits, each one of them individually are also going to see another random subset of the columns. So, now maybe the the columns that we're seeing on the first split, will most likely not be the same as the columns seen on the other two splits. And if it keeps breaking down making a very complex tree, each split is going to see just a subset of the columns. 
]

![](https://bradleyboehmke.github.io/HOML/09-random-forest_files/figure-html/tuning-mtry-1.png)


## Node size  

[Up to this point, we're talking about mtry (= the number of features each split in each tree is allowed to see). mtry does NOT determine how many breaks the tree can have. The hyperparameter that controls how deep a tree is going to be is the "node size".]

*Node size refers to the minimum number of observations in a node (of a given tree) for it to be split.*

[Smaller minimum node sizes will allow deeper tree formation ]

*Larger values of node size grow simpler individual trees, while smaller values of node size grow longer more complex trees.*  

If *computation time is a concern*, then you can often *decrease run time* substantially by *increasing the node size* and *have only marginal impacts to your error estimate*.  

![](https://bradleyboehmke.github.io/HOML/09-random-forest_files/figure-html/tuning-node-size-1.png)

## Growing an individual tree  
Random forest is a collection of many individual trees. [that's why we call it a forest] 

Each individual tree will grow based on node size, the bootstrapped sample, and evaluating a given number of random columns at each split.  

On each split, the selected variable and its binary split are chosen by evaluating many random splits for each of the randomly selected variables at that node, and choosing the variable and split that minimizes error.  

![](https://cimentadaj.github.io/ml_socsci/03_trees_files/figure-html/unnamed-chunk-46-1.png)

The example above is the random splits being evaluated for one variable at that node.  

This process happens for all variables considered at that node, and the best combination is chosen to perform the split.  

## From trees to a forest  
Random forest is an ensemble of many trees. 

Each individual tree is made up from:  
  - a bootstrapped sample of the rows  
  - at each split, a random sample of the columns  

Then the forest grows by growing many individual trees. [and combining all of them into 1 algorithm] 
When predicting, each tree makes its own prediction.  

The prediction from all trees is aggregated (averaged if regression) and the random forest provides one single predicted value.  

## Pros vs. cons of RF  
Pros:  
  - Good performance even without tuning  [very robust because of the 2 random processes]
  - Simple to train  [does not have a lot of hyperparameters]
  - It can model non-linear relationships  [because it is a tree]
  - Can be used with both numerical and categorical response variables  
  - It handles NAs  [some algorithms do not handle NAs at all, we have to impute the NAs. But, tree-based algorithms like random forest normally handle NAs pretty well]
  - Offers great balance of variance (by growing many trees) and bias (by averaging over them)  [this is the most important advantage of random forest:  imagine that a single given tree (like we saw with CIT) can have high variance because if you change the data that you train it, it may change the model completely (not completely, but to some extent it's going to change the model). In random forest, individual trees have high variance, but because you create many trees, and then you average them, you decrease that variance when you take the average. And it also balances the bias because you're averaging over them. So, the many individual trees that we grow have high variance on their own.  But when you average over them, you reduce the variance, and have a better balance between variance and bias.]
  
Cons:  
  - Not as interpretable as previous models (because there is no single tree model to look at like CIT)     
  
    
# Setup  
```{r}
#| message: false
#| warning: false

#install.packages("ranger") #one of the packages in R that models random forest

library(tidymodels)
library(tidyverse) #takes a while to load
library(vip)
library(ranger)
library(finetune)
```

```{r weather}
weather <- read_csv("../data/weather_monthsum.csv")

weather
```

# ML workflow  
We're going to use the same workflow as we used for elastic net.   

## 1. Pre-processing  
Here's where we perform **data split** and **data processing**.  

### a. Data split  
For data split, let's use **70% training / 30% testing**.

```{r weather_split}
# Setting seed to get reproducible results  
set.seed(931735)

# Setting split level  
weather_split <- initial_split(weather, 
                               prop = .7,
                               strata = strength_gtex) # strata = strength_gtex: to do stratified sampling based on strength_gtex to make sure the distribution of strength across training and testing are similar

weather_split
```


```{r weather_train}
# Setting train set 
weather_train <- training(weather_split)

weather_train
```
How many observations?

```{r weather_test}
# Setting test split
weather_test <- testing(weather_split)

weather_test
```
How many observations?  

Let's check the distribution of our predicted variable **strength_gtex** across training and testing: 
```{r distribution}
ggplot() +
  geom_density(data = weather_train, 
               aes(x = strength_gtex),
               color = "red") +
  geom_density(data = weather_test, 
               aes(x = strength_gtex),
               color = "blue") 
  
```

Now, we put our **test set** aside and continue with our **train set** for training.  

  
### b. Data processing  
Before training, we need to perform some processing steps, like  
  - normalizing  
  - **removing unimportant variables**  
  - dropping NAs  
  - performing PCA on the go  
  - removing columns with single value  
  - others?  

For that, we'll create a **recipe** of these processing steps. 

This recipe will then be applied now to the **train data**, and easily applied to the **test data** when we bring it back at the end.

Creating a recipe is an easy way to port your processing steps for other data sets without needing to repeat code, and also only considering the data it is being applied to.  

You can find all available recipe step options here: https://tidymodels.github.io/recipes/reference/index.html

Different model types require different processing steps.  
Let's check what steps are required for an elastic net model (linear_reg).
We can search for that in this link: https://www.tmwr.org/pre-proc-table  


[Note:
From Table A.1 (https://www.tmwr.org/pre-proc-table), we see that imputation is a required data pre-processing step for rand_forest() [random forest] if NAs are present in the data set. In our case, there is not NAs in our data set, so imputation is not required in our case. 

The needed preprocessing for these models depends on the implementation. Specifically:  

  - Theoretically, any tree-based model does not require imputation. However, many tree ensemble implementations (like random forest) require imputation.  
  
  - While tree-based boosting methods generally do not require the creation of dummy variables, models using the xgboost engine do. ["random forest" is a tree-based bagging method]   

- Random forest = tree-based *bagging* method
- xgboost = tree-based *boosting* method
]

> Differently from elastic net, variables do not need to be normalized in random forest, so we'll skip this step.   

```{r weather_recipe}
weather_recipe <-
  # Defining predicted and predictor variables
  recipe(strength_gtex ~ .,
         data = weather_train) %>%
  # Removing year and site  
    step_rm(year, site, matches("Jan|Feb|Mar|Apr|Nov|Dec")) #%>%
  # Normalizing all numeric variables except predicted variable
  #step_normalize(all_numeric(), -all_outcomes())

weather_recipe
```

Now that we have our recipe ready, we **need to apply it** to the training data in a process called prepping: [Note: To apply recipe to the training data, we use prep() function]

```{r weather_prep}
weather_prep <- weather_recipe %>%
  prep()

weather_prep
```


Now, we're ready to start the model training process!

## 2. Training  
### a. Model specification  
First, let's specify:  
  - the **type of model** we want to train  
  - which **engine** we want to use  
  - which **mode** we want to use  

> Elastic nets can only be run for a numerical response variable. [Conditional inference tree and] Random forests can be run with either numerical (regression) or categorical (classification) explanatory variable. Therefore, we have the need to specify the mode here. [Note: We only need to specify "mode" for models that can run both regression and classification such as CIT and random forest. Elastic net cannot do a classification model type (it can only do regression model type), so we did not specify "mode" for elastic net.]

Random forest **hyperparameters**:  
  - **trees**: number of trees in the forest    
  - **mtry**: number of random features [or columns] sampled at each node split    
    - **min_n**: minimum number of data points in a node that are required for the node to be split further  [Important note: we are not gonna fine tune "min_n" in this exercise because of time constraint, but we should fine tune "min_n" to do random forest in our own data]

Let's create a model specification that will **fine-tune** the first two for us.

A given model type can be fit with different engines (e.g., through different packages). Here, we'll use the **ranger** engine/package.  
  
```{r rf_spec}

rf_spec <- 
  # Specifying random forest as our model type, asking to tune the hyperparameters
  rand_forest(trees = tune(),
              mtry = tune() #both "trees" and "mtry" are model-type level hyperparameters, meaning that we fine tune "trees" and "mtry" inside "rand_forest()"
              ) %>%
    # Specify the engine (= package)
    set_engine("ranger") %>% #specifying "ranger" as the engine/package to run random forest 
    # Specifying mode  
    set_mode("regression") #random forest can handle both regression (when y is numerical) and classification (when y is categorical) #Here, we are specifying "set_mode("regression")" because our y variable is numerical [continuous]

rf_spec

```
Ask Dr. Bastos: For fine tuning "min_n", do we need to fine tune it at the model-type level [i.e., inside  rand_forest()], or do we need to fine tune it at the set_engine level?

Notice how the main arguments above do not have a value **yet**, because they will be tuned.  

### b. Hyper-parameter tuning  
For our iterative search, we need:  
  - Our model specification (`rf_spec`)  
  - The recipe (`weather_recipe`)  
  - Our **resampling strategy** (don't have yet) 
  

> Notice that for "random forest", we do not need to specify the parameter information, as we needed for CIT. The reason is that for random forest, all hyperparameters to be tuned are specified at the model level, whereas for CIT: one hyperparameter was at model level ["tree_depth" was fine-tuned inside decision_tree() as:  decision_tree(tree_depth = tune()) ] and another hyperparameter was at the engine level ["conditional_min_criterion" was fine tuned inside set_engine() as: set_engine("partykit", conditional_min_criterion = tune())]. Engine level hyperparameters need to be "finalized" and have their range of values set up before they can be used in search methods.  

[Note: CIT had one hyperparameter at the level of the model, and another hyperparameter at the level of the engine. Because of that, we had to do 1 extra step for the hyperparameter at the level of the engine to set the levels that would be searched in the grid. In random forest, both of the hyperparameters are fine tuned at the level of the model i.e., inside "rand_forest( trees = tune(), mtry = tune() )", and we don't have any hyperparameter at the engine level. Therefore, we can skip one of the steps here in random forest that were required for CIT.] 

> We used 10-fold CV before. It took about 10-min to run the grid on my side, so to avoid a long wait time in class, let's switch to 5-fold CV this time around. But we always need to do at least 10 folds of our data.

Let's define our resampling strategy below, using a 5-fold cross validation approach:  
```{r resampling_foldcv}
set.seed(34549)
resampling_foldcv <- vfold_cv(weather_train, 
                              v = 5) #but at least 10 folds are recommended

resampling_foldcv
resampling_foldcv$splits[[1]]
resampling_foldcv$splits[[2]]
```
On each fold, we'll use **389** observations for training and **98** observations to assess performance.    

[On each fold, we have 80% of the data on training, and 20% on assessment.]

Now, let's perform the search below.  

We will use an iterative search algorithm called **simulated annealing**.  

Here's how it works:  
![](https://www.tmwr.org/figures/iterative-neighborhood-1.png)
  - In the example above, mixture and penalty from an elastic net model are being tuned.  

  - It finds a candidate value of hyperparameters and their associated rmse to start (iteration 1).  

  - It establishes a radius around the first proposal, and randomly chooses a new set of values within that radius.  
  
  - If this achieves better results than the previous parameters, it is accepted as the new best and the process continues. If the results are worse than the previous value the search procedure may still use this parameter to define further steps. 
  
  - After a given number of iterations, the algorithm stops and provides a list of the best models and their hyperparameters.  

In the algorithm below, we are asking for 50 iterations.  

```{r rf_grid_result}
set.seed(76544)
rf_grid_result <- tune_sim_anneal(object = rf_spec,
                     preprocessor = weather_recipe,
                     resamples = resampling_foldcv,
                    #param_info = rf_param,
                    iter = 10 #we are using "iter = 10" because of time constraint in class #but at least 100 is recommended #"iter = 10" also means that the iteration is gonna run 10 times to find the best #If we specified "iter = 100", the iteration would run 100 times to find the best, running 100 iteration is the best practice
                     )

beepr::beep() #plays a bell sound when the code chunk finishes running #this comes in handy for a code chunk that takes a long time to run, because the bell sound indicates the code chunk has finished running

rf_grid_result
rf_grid_result$.metrics[[2]]
```
Notice how we have a column for iterations.  
The first iteration uses a sensible value for the hyper-parameters, and then starts "walking" the parameter space in the direction of greatest improvement.  

Let's collect a summary of metrics (across all folds, for each iteration), and plot them.  

Firs, RMSE (lower is better):
```{r RMSE}
rf_grid_result %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  ggplot(aes(x = mtry, 
             y = trees 
             )) +
  geom_path(group = 1) +
  geom_point(aes(color = mean),
             size = 3) + 
  scale_color_viridis_b() +
  geom_text(aes(label = .iter), nudge_x = .0005) +
  labs(title = "RMSE")
```

We want to minimize the RMSE. The lower the RMSE, the better. In the legend section of the RMSE grid plot seen above, the lowest RMSE is"2.85" which has a dark purple color, and we see the dark purple colored dot is on 4 th iteration, which indicates that the lowest RMSE [= "2.85"] was obtained in the 4 th iteration.


What tree_depth and mtry values created lowest RMSE?  

tree_depth = ~ 110; mtry = ~ 22 [when we have 10 iterations]

Now, let's look into R2 (higher is better):  

```{r R2}
rf_grid_result %>%
  collect_metrics() %>%
  filter(.metric == "rsq") %>%
  ggplot(aes(x = mtry, 
             y = trees 
             )) +
  geom_path(group = 1) +
  geom_point(aes(color = mean),
             size = 3) + 
  scale_color_viridis_b() +
  geom_text(aes(label = .iter), nudge_x = .0005) +
  labs(title = "R2")

```

We want to see higher R2 (R2: higher is better).

We want to see a maximized R2. The higher the R2, the better. In the legend section of the R2 grid plot seen above, the highest R2 is "0.16" which has a yellow color, and we see the yellow colored dot is on 4 th iteration, which indicates that the highest R2 [= "0.16"] was obtained in the 4 th iteration.

What tree_depth and min criterion values created highest R2?  

tree_depth = ~110; mtry = ~ 22 [this was the best result for 10 iterations]

> Previously, we selected the single best model. Now, let's select the best model by "percent loss" of the metric, so we choose a model among the top ones that is more parsimonious.  

```{r}
# Based on lowest RMSE
best_rmse <- rf_grid_result %>%
  select_by_pct_loss("trees",
                     metric = "rmse",
                     limit = 2 #"limit = 2": this is how many percent losses you are accepting, 2% in this case
                     )

best_rmse

```

Ask Dr. Bastos: 

In the class, it was mentioned that "limit = 2": this is how many percent losses you are excepting in this case. Why are we using "limit = 2" instead of "limit = 1", because wouldn't that be accepting 2% losses? What is the best practice when it comes to "percent loss of the metric": using 1%, or 2%?


```{r}
# Based on greatest R2
best_r2 <- rf_grid_result %>%
  select_by_pct_loss("trees",
                     metric = "rsq",
                     limit = 2
                     )


best_r2

```

I got the he following results that were different than Dr. Bastos's results (because of the randomness of 2 hyperparameters going on at each tree for a lot of models):

Based on RMSE, we would choose   
  - mtry = 25 
  - trees = 443

Based on R2, we would choose   
  - mtry = 22
  - trees = 113


However, Dr. Bastos got the following results:

Based on RMSE, we would choose   
  - mtry = 24 
  - trees = 518

Based on R2, we would choose   
  - mtry = 25
  - trees = 273

Let's use the hyperparameter values that optimized R2 to fit our final model.

```{r final_spec}
final_spec <- rand_forest(trees = best_r2$trees,
                          mtry = best_r2$mtry) %>%
  # Specify the engine
  set_engine("ranger",
             importance = "permutation" #"permutation" is how we look into variable importance in random forest
             ) %>%
    # Specifying mode  
  set_mode("regression")
  

final_spec
```

## 3. Validation  

Now that we determined our best model, let's do our **last fit**.

### What does last fit mean? (Will come in exam)

This means 2 things:  
  - Training the optimum hyperparameter values on the **entire training set**  
  - Using it to **predict** on the **test set**  

These 2 steps can be completed in one function, as below:  

```{r final_fit}
final_fit <- last_fit(final_spec, 
                weather_recipe,
                split = weather_split)

final_fit %>%
  collect_predictions()
```

Metrics on the **test set**:
```{r}
final_fit %>%
  collect_metrics()
```

From the output,
Fit metrics on the test set:
rmse = 2.9317445	
rsq	= 0.1882245	


Metrics on **train set** (for curiosity and compare to test set):  
```{r}
# RMSE
final_spec %>%
  fit(strength_gtex ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rmse(strength_gtex, .pred) %>%
  bind_rows(
    
    
    # R2
    final_spec %>%
      fit(strength_gtex ~ .,
          data = bake(weather_prep, 
                      weather_train)) %>%
      augment(new_data = bake(weather_prep, 
                              weather_train)) %>% 
      rsq(strength_gtex, .pred)
    
  )

```

From the output,
Fit metrics on the train set:
rmse = 1.1385078	
rsq	= 0.9470296	

How does metrics on test compare to metrics on train?  

Fit metrics on the test set:
rmse = 2.9317445	
rsq	= 0.1882245	

Fit metrics on the train set:
rmse = 1.1385078	
rsq	= 0.9470296	

We see that the R2 is really r-e-a-l-l-y high in the train set (rsq	= 0.9470296), as compared to the R2 of the test set 0.1882245. The RMSE is lower in the train set (rmse = 1.1385078) as compared to the RMSE of the test set (rmse = 2.9317445). [This is the reason why it's okay to report the fit metrics of the train set to understand how well our model is explaining our data, but it is not a good measure of the predictive ability of the model. To understand the predictive ability of the model, we have to look at the fit metrics of the test set. Notice that the R2 of the test set was 0.1882245, whereas the R2 of the train set was 0.9470296 -- which is a very large difference. Hence, *we need to always report the fit metrics of the test set* along with the fit metrics of the train set. ]

Predicted vs. observed plot:  
```{r}
final_fit %>%
  collect_predictions() %>%
  ggplot(aes(x = strength_gtex,
             y = .pred)) +
  geom_point() +
  geom_abline() +
  geom_smooth(method = "lm") +
  scale_x_continuous(limits = c(20, 40)) +
  scale_y_continuous(limits = c(20, 40)) 
```

[**Very important note:**
To interpret random forest model, we should always expcet to see (1) *perdicted vs. observed plot* and (3) *variable importance plot*, at the least -- these are the 2 things that must be mentioned in a random forest model. 
]

*Variable importance:* 

[For Random Forest,] The importance metric we are evaluating here is **permutation**. 

In the permutation-based approach, for each tree, the out-of-bag sample is passed down the tree and the prediction accuracy is recorded. [as is]   

Then the values for each variable (one at a time) are randomly permuted and the accuracy is again computed.   

The decrease in accuracy as a result of this randomly shuffling of feature values is averaged over all the trees for each predictor.   

The variables with the **largest average decrease in accuracy** [after being permuted] are [the ones that are] considered **most important**.  

```{r}

final_spec %>%
  fit(strength_gtex ~ .,
         data = bake(weather_prep, weather)) %>%
    vi() %>%
  mutate(
    Variable = fct_reorder(Variable, 
                           Importance)
  ) %>%
  ggplot(aes(x = Importance, 
             y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)
    
```

**Therefore, solar radiation in July, the minimum temperature in June  [Dr. Bastos found solar radiation in August to be the 2nd most important variable], and solar radiation in "August" [Dr. Bastos found minimum temperature in June to be the 3rd most important variable] were the most important variables affecting cotton fiber strength.**  

[Note:

*Difference in the variable importance plot between CIT and random forest:*

In random forest, all predictor variables are gonna have certain level of importance in contrast to CIT. That is why we see all predictors variables appearing in the variable importance plot of random forest (as seen above).

However, in CIT, the only important variables are the ones that appeared in the single tree. That's why only 3 predictor variables showed up in the variable importance plot of CIT.

But in random forest, we have about 500 trees, and each one of these trees have different predictor variables. So, when each of these predictor variables are shuffled, it's assessing which variables after shuffling reduce the model accuracy significantly. 
]

# Summary  
In this exercise, we covered: 
  - Random forest algorithm    
  - Set up a ML workflow to train an rf model  
  - Used `recipes` to process data
  - Used `rsamples` to split data  
  - Used **iterative search** to find the best values for max_depth and mtry   
  - Used 5-fold cross validation as the resampling method  
  - Used both R2 and RMSE as the metrics to select best model  (within 2 percent loss)
  - Once final model was determined, used it to predict **test set**  
  - Evaluated it with predicted vs. observed plot, R2 and RMSE metrics, variable importance plot, and tree plot
  
[**Very important note:**
In random forest, we do not have a single tree (which was the case in CIT), but about 500 trees in this case.

So, in order to interpret random forest models, we should always expcet to see (1) *perdicted vs. observed plot* and (2) *variable importance plot*, at the least -- these are the 2 things that must be mentioned in a random forest model. 
]


